{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for Ridesharing\n",
    "\n",
    "\n",
    "In this notebook, we will walk through the ridesharing problem with three different enviornment configurations that is set up in the package. \n",
    "\n",
    "The following two variables can be used to 1) choose between the two version of the environment (no travel_time vs travel_time) and 2) use grid approximation to optimize the $\\alpha$ parameter of the max_weight agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_travel_time = False\n",
    "algo_tune_on = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Package Installation\n",
    "First we import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of $K4$ graph with uniform length of edges\n",
    "The first configuration consists of a simple $K4$ graph with uniform distance of 10 for all edges. For this particular set-up, the closest_car agent trumps the max_weight agent in performance. Max_weight agent's algorithm attempts to maintain a relative uniform distribution throughout the system, allowing any incoming request to have a car readily available close by at all times. However, for complete graphs with uniformly distanced edges, as distance from one node to another is always uniform and any node with a car can be used to dispatch, max_weight agent's use of the weight parameter $\\alpha$ and number of cars available at each node outputs less optimal action, compared to the closest_car agent.\n",
    "The details of the configuration is specified below.\n",
    "\n",
    "\n",
    "* The network is a $K4$ graph with uniform distance of 10 for all edges.\n",
    "* There are 10 avaiable cars in the system.\n",
    "* The fare parameter $f$ is 3, and the cost parameter is $c$ is 1.\n",
    "* The average velocity $v$ is 3.\n",
    "* $\\gamma$ and $d_{threshold}$ are 1 and 20 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Pick problem parameters for the environment\n",
    "\n",
    "Here we use the ridesharing environment as outlined in `or_suite/envs/ambulance/ambulance_metric.py`. In addition, we need to specify the number of episodes for learning, and the number of iterations (in order to plot average results with confidence intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG =  or_suite.envs.env_configs.rideshare_graph_2cities_config\n",
    "CONFIG['epLen'] = 10\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 1000\n",
    "numIters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pick simulation parameters\n",
    "\n",
    "Next we need to specify parameters for the simulation. This includes setting a seed, the frequency to record the metrics, directory path for saving the data files, a deBug mode which prints the trajectory, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/rideshare/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "starting_state = CONFIG['starting_state']\n",
    "num_cars = CONFIG['num_cars']\n",
    "num_nodes = len(starting_state)\n",
    "\n",
    "if has_travel_time:\n",
    "  rideshare_env = gym.make('Rideshare-v1', config=CONFIG) # fix the indents...\n",
    "else:\n",
    "  rideshare_env = gym.make('Rideshare-v0', config=CONFIG)\n",
    "mon_env = Monitor(rideshare_env)\n",
    "\n",
    "scaling_list = [0.01, 0.1, 1., 10.]\n",
    "observation_space = rideshare_env.observation_space\n",
    "action_space = rideshare_env.action_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Pick list of algorithms\n",
    "\n",
    "We have several heuristics implemented for each of the environments defined, in addition to a `Random` policy, and some `RL discretization based` algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4, 4, 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 4, 4, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "agents = { #'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "#'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "#'maxweightfixed' : or_suite.agents.rideshare.max_weight_fixed.maxWeightFixedAgent(CONFIG['epLen'], CONFIG, [1 for _ in range(num_nodes)]),\n",
    "'closestcar' : or_suite.agents.rideshare.closest_car.closetCarAgent(CONFIG['epLen'], CONFIG),\n",
    "'randomcar' : or_suite.agents.rideshare.random_car.randomCarAgent(CONFIG['epLen'], CONFIG),\n",
    "'DiscreteQL' : or_suite.agents.rl.discrete_ql.DiscreteQl(action_space, observation_space, epLen, scaling_list[0]),\n",
    "'DiscreteMB': or_suite.agents.rl.discrete_mb.DiscreteMB(action_space, observation_space, epLen, scaling_list[0], 0, False)\n",
    "}\n",
    "\n",
    "#param_list = [list(p) for p in it.product(np.linspace(0,1,4),repeat = len(starting_state))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscreteMB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/43sgrxps7lbcv1k2xnpw4g5h0000gn/T/ipykernel_39569/2045633810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single_sb_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmon_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_SETTINGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrideshare_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_SETTINGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpath_list_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/rideshare_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/ORSuite/or_suite/utils.py\u001b[0m in \u001b[0;36mrun_single_algo\u001b[0;34m(env, agent, settings)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/ORSuite/or_suite/experiment/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;31m# steps based on the action chosen by the algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mnewState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0mepReward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/ORSuite/or_suite/envs/ridesharing/rideshare_graph.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_dist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0maccept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;31m# print(\"prob: \" + str(prob))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# print(\"accept: \" + str(accept))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36many\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "linspace_alpha = []\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/rideshare_'+str(agent)+'_'+str(num_cars)\n",
    "   # if algo_tune_on and agent == 'maxweightfixed':\n",
    "   #     or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], param_list, DEFAULT_SETTINGS)\n",
    "    if algo_tune_on and agent == 'DiscreteQL':\n",
    "        or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    if algo_tune_on and agent == 'DiscreteMB':\n",
    "        or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(rideshare_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "        algo_list_radar.append(str(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generate Figures\n",
    "\n",
    "Create a chart to compare the different heuristic functions. The ridesharing environment offers three more metrics: acceptance rate, mean and variance of response time. They are named as ACPT, MN, and VAR, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Algorithm    Reward      Time    Space      ACPT          MN        VAR\n",
      "0  closestcar  9.998924  5.453197  -9871.2  1.000000  [-0.81076]  -6.771578\n",
      "1   randomcar  9.998634  5.005686  -9930.2  0.999970  [-5.03099] -26.060830\n",
      "2  DiscreteQL  8.498688  4.935753 -11482.4  0.813465  [-5.09903] -25.762623\n",
      "3  DiscreteMB  8.948711  4.379160 -11423.2  0.840285  [-5.06364] -25.805790\n"
     ]
    }
   ],
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'rideshare_'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {'ACPT': lambda traj : or_suite.utils.acceptance_rate(traj, lambda x, y : lengths[x,y]),'MN': lambda traj : or_suite.utils.mean_dispatch_dist(traj, lambda x, y : lengths[x,y]),'VAR': lambda traj : or_suite.utils.var_dispatch_dist(traj, lambda x, y : lengths[x,y])}\n",
    "\n",
    "graph = nx.Graph(CONFIG['edges'])\n",
    "lengths = or_suite.envs.ridesharing.rideshare_graph.RideshareGraphEnvironment.find_lengths(rideshare_env, graph, graph.number_of_nodes())\n",
    "\n",
    "fig_name = 'rideshare_'+'_'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42d2a0fd935109bf7af0945201a4df1598f849f59a8941473aae5806272937b6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
