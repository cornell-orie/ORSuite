{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "# Resource Allocation Code Demo\n",
    "\n",
    "The Food Bank of the Southern Tier (FBST) is a member of Feeding America, focused on providing food security for people with limited financial resources, and serves six counties and nearly 4,000 square miles in the New York.  Under normal operations (non COVID times), the Mobile Food Pantry program is among the main activities of the FBST.  The goal of the service is to make nutritious and healthy food more accessible to people in underserved communities.  Even in areas where other agencies provide assistance, clients may not always have access to food due to limited public transportation options, or because those agencies are only open hours or days per work.\n",
    "\n",
    "Here we do a sample experiment testing out some of the existing and developed algorithms against a randomized heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54262089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-roots",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting out configuration parameter for the environment\n",
    "# CONFIG = or_suite.envs.env_configs.resource_allocation_foodbank_config(5)\n",
    "CONFIG = or_suite.envs.env_configs.resource_allocation_default_config\n",
    "\n",
    "\n",
    "# Specifying training iteration, epLen, number of episodes, and number of iterations\n",
    "epLen = CONFIG['num_rounds']\n",
    "nEps = 1\n",
    "numIters = 100\n",
    "\n",
    "# Configuration parameters for running the experiment\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/resource/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, # save trajectory for calculating additional metrics\n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False # indicator for pickling final information\n",
    "                    }\n",
    "\n",
    "resource_env = gym.make('Resource-v0', config=CONFIG)\n",
    "mon_env = Monitor(resource_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-amplifier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and variance endomwnets:\n",
      "[[1.98  1.979 2.007 1.943 1.991 2.071 1.984 2.002 2.083 1.956]\n",
      " [2.994 3.01  2.993 2.941 3.014 2.976 3.046 3.006 3.    3.06 ]\n",
      " [3.99  3.957 3.929 4.106 3.957 3.988 4.023 3.978 3.991 3.967]] [[0.9996   0.932559 1.048951 0.937751 0.960919 1.081959 0.997744 0.949996\n",
      "  1.076111 0.962064]\n",
      " [2.109964 1.9739   1.920951 1.879519 2.085804 2.001424 2.147884 1.807964\n",
      "  2.106    2.1904  ]\n",
      " [3.0639   3.025151 2.989959 3.012764 2.803151 2.897856 3.034471 2.897516\n",
      "  2.848919 2.967911]]\n",
      "Mean and variance endomwnets:\n",
      "[[1.971 1.991 2.01  2.002 2.037 1.984 1.996 1.976 2.014 1.985]\n",
      " [3.018 2.97  2.933 3.042 3.02  2.981 2.974 3.001 3.048 3.057]\n",
      " [3.926 3.981 4.033 4.046 3.912 4.045 3.928 4.032 4.014 4.094]] [[0.954159 0.942919 1.0879   0.953996 0.989631 1.011744 0.945984 0.999424\n",
      "  1.019804 1.030775]\n",
      " [2.059676 2.0671   1.886511 2.064236 2.1316   2.064639 2.095324 2.134999\n",
      "  2.195696 1.985751]\n",
      " [3.034524 3.074639 2.967911 2.703884 3.146256 3.138975 2.946816 2.838976\n",
      "  2.979804 3.057164]]\n",
      "Mean and variance endomwnets:\n",
      "[[2.005 2.039 2.044 2.022 1.994 1.934 1.971 1.952 1.978 2.019]\n",
      " [2.96  3.003 3.003 3.057 2.967 3.025 3.011 3.006 2.989 2.99 ]\n",
      " [3.967 3.905 4.034 3.987 3.994 4.032 4.075 3.904 4.025 3.97 ]] [[1.010975 0.981479 1.050064 1.069516 1.003964 0.905644 0.920159 0.897696\n",
      "  0.969516 1.024639]\n",
      " [2.0384   2.114991 1.966991 1.855751 1.921911 2.006375 2.072879 1.915964\n",
      "  2.020879 2.0459  ]\n",
      " [2.921911 2.983975 3.262844 2.788831 2.973964 2.854976 3.121375 2.900784\n",
      "  3.084375 2.9011  ]]\n"
     ]
    }
   ],
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "#  'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "#  'Equal': or_suite.agents.resource_allocation.equal_allocation.equalAllocationAgent(epLen, CONFIG),\n",
    "#  'FixedThreshold': or_suite.agents.resource_allocation.fixed_threshold.fixedThresholdAgent(epLen, CONFIG),\n",
    " 'Guardrail-0.5': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.5),\n",
    " 'Guardrail-0.3': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.3),\n",
    " 'Guardrail-0.25': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.25)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-dublin",
   "metadata": {},
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e2e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a07ba9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail-0.5\n",
      "Lower and Upper Solutions:\n",
      "[[ 0.       0.13359]\n",
      " [-0.       0.13359]\n",
      " [ 0.16428  0.     ]]\n",
      "[[0.      0.19537]\n",
      " [0.      0.19537]\n",
      " [0.24026 0.     ]]\n",
      "Writing to file data.csv\n",
      "Guardrail-0.3\n",
      "Lower and Upper Solutions:\n",
      "[[ 0.       0.13324]\n",
      " [-0.       0.13324]\n",
      " [ 0.16294  0.     ]]\n",
      "[[0.      0.26711]\n",
      " [0.      0.26711]\n",
      " [0.32665 0.     ]]\n",
      "Writing to file data.csv\n",
      "Guardrail-0.25\n",
      "Lower and Upper Solutions:\n",
      "[[0.      0.13232]\n",
      " [0.      0.13232]\n",
      " [0.16241 0.     ]]\n",
      "[[0.      0.30234]\n",
      " [0.      0.30234]\n",
      " [0.37109 0.     ]]\n",
      "Writing to file data.csv\n",
      "        Algorithm    Reward      Time     Space  Efficiency  Hindsight Envy  \\\n",
      "0   Guardrail-0.5 -7.180845  5.202982 -14853.74 -220.793910     -156.601470   \n",
      "1   Guardrail-0.3 -7.153558  5.205476 -12642.07 -175.641999     -224.420292   \n",
      "2  Guardrail-0.25 -7.340392  5.199175 -12535.07 -177.091640     -256.127386   \n",
      "\n",
      "   Counterfactual Envy  Budget  \n",
      "0           -44.538440     0.0  \n",
      "1           -75.136680     0.0  \n",
      "2           -93.111919     0.0  \n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/resource_'+str(agent)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(resource_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(resource_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/resource_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/resource_'+str(agent)+'/')\n",
    "        algo_list_radar.append(str(agent))     \n",
    "        \n",
    "fig_path = '../figures/'\n",
    "fig_name = 'resource'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)        \n",
    "        \n",
    "additional_metric = {'Efficiency': lambda traj : or_suite.utils.delta_EFFICIENCY(traj, CONFIG), \\\n",
    "                      'Hindsight Envy': lambda traj : or_suite.utils.delta_HINDSIGHT_ENVY(traj, CONFIG), \\\n",
    "                      'Counterfactual Envy': lambda traj : or_suite.utils.delta_COUNTERFACTUAL_ENVY(traj, CONFIG), \\\n",
    "                      'Budget' : lambda traj : or_suite.utils.times_out_of_budget(traj, CONFIG)}\n",
    "#                       'Prop': lambda traj : or_suite.utils.delta_PROP(traj, CONFIG), \\\n",
    "#                       'Exante Envy': lambda traj : or_suite.utils.delta_EXANTE_ENVY(traj, CONFIG)}\n",
    "fig_name = 'resource'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96273425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/resource_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdf84cc91c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/resource_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c08c369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"../figures/resource_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdf84cb80d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/resource_radar_plot.pdf\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
