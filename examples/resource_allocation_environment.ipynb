{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "# Resource Allocation Code Demo\n",
    "\n",
    "The Food Bank of the Southern Tier (FBST) is a member of Feeding America, focused on providing food security for people with limited financial resources, and serves six counties and nearly 4,000 square miles in the New York.  Under normal operations (non COVID times), the Mobile Food Pantry program is among the main activities of the FBST.  The goal of the service is to make nutritious and healthy food more accessible to people in underserved communities.  Even in areas where other agencies provide assistance, clients may not always have access to food due to limited public transportation options, or because those agencies are only open hours or days per work.\n",
    "\n",
    "Here we do a sample experiment testing out some of the existing and developed algorithms against a randomized heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54262089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-roots",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting out configuration parameter for the environment\n",
    "CONFIG = or_suite.envs.env_configs.resource_allocation_foodbank_config(3)\n",
    "# CONFIG = or_suite.envs.env_configs.resource_allocation_default_config\n",
    "\n",
    "\n",
    "# Specifying training iteration, epLen, number of episodes, and number of iterations\n",
    "epLen = CONFIG['num_rounds']\n",
    "nEps = 1\n",
    "numIters = 2\n",
    "\n",
    "# Configuration parameters for running the experiment\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/resource/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, # save trajectory for calculating additional metrics\n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False # indicator for pickling final information\n",
    "                    }\n",
    "\n",
    "resource_env = gym.make('Resource-v0', config=CONFIG)\n",
    "mon_env = Monitor(resource_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-amplifier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and variance endomwnets:\n",
      "[[36.66488446 50.41276469 36.34946901]\n",
      " [43.95244355 60.70726574 43.73277552]\n",
      " [65.78668211 91.91349249 65.44240092]] [[  87.06560246  155.48197986  589.03745581]\n",
      " [ 174.328777    329.57167123  929.44619453]\n",
      " [ 795.64847456 1694.07540568 2888.09737205]]\n",
      "Mean and variance endomwnets:\n",
      "[[40.4304571   8.6044611  35.25868755]\n",
      " [48.67806343 10.52719357 42.21112546]\n",
      " [71.95769637 15.92069786 63.86573431]] [[  91.11649898    4.5873471   610.49597968]\n",
      " [ 205.52798238    9.7622034   952.33464102]\n",
      " [1026.29245453   50.02667417 2671.09559557]]\n",
      "Mean and variance endomwnets:\n",
      "[[ 66.86340481  14.03271871  34.35091664]\n",
      " [ 80.33388422  17.03301621  41.81046881]\n",
      " [120.72047828  26.21419851  62.10243205]] [[ 273.10377418   11.55578419  549.15967234]\n",
      " [ 548.18569278   26.68250428  887.71970428]\n",
      " [2859.27772253  132.96503133 2576.1310948 ]]\n"
     ]
    }
   ],
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "#  'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "#  'Equal': or_suite.agents.resource_allocation.equal_allocation.equalAllocationAgent(epLen, CONFIG),\n",
    "#  'FixedThreshold': or_suite.agents.resource_allocation.fixed_threshold.fixedThresholdAgent(epLen, CONFIG),\n",
    " 'Guardrail-0.5': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.5),\n",
    "#  'Guardrail-0.3': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.3),\n",
    "#  'Guardrail-0.25': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.25)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-dublin",
   "metadata": {},
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e2e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a07ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail-0.5\n",
      "\n",
      "env reset!\n",
      "starting_state [519.2      519.2      519.2      519.2      519.2       61.867798\n",
      "  43.10978   60.375957]\n",
      "Lower and Upper Solutions:\n",
      "[[0.20146 0.21181 0.36877 0.21525 0.     ]\n",
      " [0.32994 0.30984 0.      0.30317 0.     ]\n",
      " [0.13981 0.14636 0.24919 0.14853 0.50092]]\n",
      "[[0.47806 0.50202 0.86896 0.50996 0.     ]\n",
      " [0.78127 0.73279 0.      0.71674 0.     ]\n",
      " [0.32941 0.3459  0.592   0.35136 1.18518]]\n",
      "timestep:  0\n",
      "new state [436.0547   435.6667   429.67523  435.53772  488.904     41.256927\n",
      "  75.95979    5.776585]\n",
      "reward:  -0.6129640600744122\n",
      "timestep:  1\n",
      "new state [355.08344  357.2942   390.36688  358.02527  481.99908   48.038857\n",
      "  56.942787  82.43221 ]\n",
      "reward:  -0.41037753218464124\n",
      "timestep:  2\n",
      "new state [437.4      437.4      437.4      437.4      437.4       22.08356\n",
      "  26.83384   50.112766]\n",
      "reward:  -0.5395880389906147\n",
      "\n",
      "env reset!\n",
      "starting_state [437.4       437.4       437.4       437.4       437.4         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "timestep:  0\n",
      "new state [420.7577   420.4627   416.0367   420.36496  425.4039     6.277636\n",
      "   7.163428  17.836748]\n",
      "reward:  -0.7239442023183881\n",
      "timestep:  1\n",
      "new state [406.28442   405.89218   400.01877   405.76218   404.25742     3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "reward:  -0.5753401832768762\n",
      "timestep:  2\n",
      "new state [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "reward:  -0.5274992782905886\n",
      "Writing to file data.csv\n",
      "Guardrail-0.3\n",
      "\n",
      "env reset!\n",
      "starting_state [314.2     314.2     314.2     314.2     314.2      91.14954  63.51344\n",
      "  88.95162]\n",
      "Lower and Upper Solutions:\n",
      "[[0.27804 0.28442 0.49769 0.28633 0.     ]\n",
      " [0.44366 0.42017 0.      0.41239 0.     ]\n",
      " [0.18626 0.19734 0.31617 0.20117 0.71439]]\n",
      "[[0.99363 1.01608 1.76182 1.02257 0.     ]\n",
      " [1.5866  1.49345 0.      1.46271 0.     ]\n",
      " [0.65621 0.70242 1.13464 0.71841 2.54433]]\n",
      "timestep:  0\n",
      "new state [244.11029  244.0351   240.6802   244.01447  250.57654   60.78364\n",
      " 111.91122    8.510616]\n",
      "reward:  -1.0993782832271304\n",
      "timestep:  1\n",
      "new state [175.97429  178.04579  207.68204  178.74713  244.4103    18.102997\n",
      "  21.45836   31.063812]\n",
      "reward:  -0.9697193458080579\n",
      "timestep:  2\n",
      "new state [491.9      491.9      491.9      491.9      491.9       34.76189\n",
      "  42.239338  78.882866]\n",
      "reward:  -1.0989362256775466\n",
      "\n",
      "env reset!\n",
      "starting_state [491.9       491.9       491.9       491.9       491.9         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "timestep:  0\n",
      "new state [458.21954  457.5003   450.16006  457.25284  474.79477    6.277636\n",
      "   7.163428  17.836748]\n",
      "reward:  -0.057955805632930076\n",
      "timestep:  1\n",
      "new state [428.91174   427.89462   418.85812   427.5414    429.4055      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "reward:  0.1355008069874194\n",
      "timestep:  2\n",
      "new state [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "reward:  0.18334094940758364\n",
      "Writing to file data.csv\n",
      "Guardrail-0.25\n",
      "\n",
      "env reset!\n",
      "starting_state [314.2     314.2     314.2     314.2     314.2      91.14954  63.51344\n",
      "  88.95162]\n",
      "Lower and Upper Solutions:\n",
      "[[0.23649 0.24295 0.42835 0.24494 0.     ]\n",
      " [0.37989 0.35941 0.      0.35261 0.     ]\n",
      " [0.16004 0.16909 0.27123 0.17222 0.6084 ]]\n",
      "[[0.99188 1.01709 1.7632  1.02452 0.     ]\n",
      " [1.58867 1.49321 0.      1.46197 0.     ]\n",
      " [0.65598 0.70203 1.14542 0.71775 2.53329]]\n",
      "timestep:  0\n",
      "new state [254.28012  254.18704  250.998    254.15912  260.00452   60.78364\n",
      " 111.91122    8.510616]\n",
      "reward:  -1.255250879681928\n",
      "timestep:  1\n",
      "new state [196.02942  197.75859  222.59705  198.34406  254.74031   18.102997\n",
      "  21.45836   31.063812]\n",
      "reward:  -1.1255908031285535\n",
      "timestep:  2\n",
      "new state [491.9      491.9      491.9      491.9      491.9       34.76189\n",
      "  42.239338  78.882866]\n",
      "reward:  -1.2548098839289752\n",
      "\n",
      "env reset!\n",
      "starting_state [491.9       491.9       491.9       491.9       491.9         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "timestep:  0\n",
      "new state [458.2268   457.50275  449.89062  457.25705  477.33148    6.277636\n",
      "   7.163428  17.836748]\n",
      "reward:  -0.06770072638103418\n",
      "timestep:  1\n",
      "new state [428.91928   427.89938   418.38776   427.55045   432.1391      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "reward:  0.1358669488674694\n",
      "timestep:  2\n",
      "new state [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "reward:  0.1837069977494408\n",
      "Writing to file data.csv\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 0\n",
      "oldState [519.2      519.2      519.2      519.2      519.2       61.867798\n",
      "  43.10978   60.375957]\n",
      "action [[4.7806e-01 5.0202e-01 8.6896e-01 5.0996e-01 5.0000e-04]\n",
      " [7.8127e-01 7.3279e-01 5.0000e-04 7.1674e-01 5.0000e-04]\n",
      " [3.2941e-01 3.4590e-01 5.9200e-01 3.5136e-01 5.0092e-01]]\n",
      "reward -0.6129640600744122\n",
      "newState [436.0547   435.6667   429.67523  435.53772  488.904     41.256927\n",
      "  75.95979    5.776585]\n",
      "info {'type': array([41.25692587, 75.95978972,  5.77658511])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 1\n",
      "oldState [436.0547   435.6667   429.67523  435.53772  488.904     41.256927\n",
      "  75.95979    5.776585]\n",
      "action [[4.78060e-01 5.02020e-01 8.68960e-01 5.09960e-01 5.00000e-04]\n",
      " [7.81270e-01 7.32790e-01 5.00000e-04 7.16740e-01 5.00000e-04]\n",
      " [3.29410e-01 3.45900e-01 5.92000e-01 3.51360e-01 1.18518e+00]]\n",
      "reward -0.41037753218464124\n",
      "newState [355.08344  357.2942   390.36688  358.02527  481.99908   48.038857\n",
      "  56.942787  82.43221 ]\n",
      "info {'type': array([48.03885764, 56.94278883, 82.43221248])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 2\n",
      "oldState [355.08344  357.2942   390.36688  358.02527  481.99908   48.038857\n",
      "  56.942787  82.43221 ]\n",
      "action [[4.78060e-01 5.02020e-01 8.68960e-01 5.09960e-01 5.00000e-04]\n",
      " [7.81270e-01 7.32790e-01 5.00000e-04 7.16740e-01 5.00000e-04]\n",
      " [3.29410e-01 3.45900e-01 5.92000e-01 3.51360e-01 1.18518e+00]]\n",
      "reward -0.5395880389906147\n",
      "newState [437.4      437.4      437.4      437.4      437.4       22.08356\n",
      "  26.83384   50.112766]\n",
      "info {'type': array([22.08356051, 26.833839  , 50.1127669 ])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 0\n",
      "oldState [437.4       437.4       437.4       437.4       437.4         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "action [[4.7806e-01 5.0202e-01 8.6896e-01 5.0996e-01 5.0000e-04]\n",
      " [7.8127e-01 7.3279e-01 5.0000e-04 7.1674e-01 5.0000e-04]\n",
      " [3.2941e-01 3.4590e-01 5.9200e-01 3.5136e-01 5.0092e-01]]\n",
      "reward -0.7239442023183881\n",
      "newState [420.7577   420.4627   416.0367   420.36496  425.4039     6.277636\n",
      "   7.163428  17.836748]\n",
      "info {'type': array([ 6.27763623,  7.16342792, 17.83674722])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 1\n",
      "oldState [420.7577   420.4627   416.0367   420.36496  425.4039     6.277636\n",
      "   7.163428  17.836748]\n",
      "action [[4.78060e-01 5.02020e-01 8.68960e-01 5.09960e-01 5.00000e-04]\n",
      " [7.81270e-01 7.32790e-01 5.00000e-04 7.16740e-01 5.00000e-04]\n",
      " [3.29410e-01 3.45900e-01 5.92000e-01 3.51360e-01 1.18518e+00]]\n",
      "reward -0.5753401832768762\n",
      "newState [406.28442   405.89218   400.01877   405.76218   404.25742     3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "info {'type': array([3.24394553, 4.69108711, 5.75323122])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 2\n",
      "oldState [406.28442   405.89218   400.01877   405.76218   404.25742     3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "action [[4.78060e-01 5.02020e-01 8.68960e-01 5.09960e-01 5.00000e-04]\n",
      " [7.81270e-01 7.32790e-01 5.00000e-04 7.16740e-01 5.00000e-04]\n",
      " [3.29410e-01 3.45900e-01 5.92000e-01 3.51360e-01 1.18518e+00]]\n",
      "reward -0.5274992782905886\n",
      "newState [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "info {'type': array([ 91.38704446, 116.69336287,  95.54699897])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 0\n",
      "oldState [314.2     314.2     314.2     314.2     314.2      91.14954  63.51344\n",
      "  88.95162]\n",
      "action [[2.7804e-01 2.8442e-01 4.9769e-01 2.8633e-01 5.0000e-04]\n",
      " [4.4366e-01 4.2017e-01 5.0000e-04 4.1239e-01 5.0000e-04]\n",
      " [1.8626e-01 1.9734e-01 3.1617e-01 2.0117e-01 7.1439e-01]]\n",
      "reward -1.0993782832271304\n",
      "newState [244.11029  244.0351   240.6802   244.01447  250.57654   60.78364\n",
      " 111.91122    8.510616]\n",
      "info {'type': array([ 60.78364135, 111.91121293,   8.51061659])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 1\n",
      "oldState [244.11029  244.0351   240.6802   244.01447  250.57654   60.78364\n",
      " 111.91122    8.510616]\n",
      "action [[2.7804e-01 2.8442e-01 4.9769e-01 2.8633e-01 5.0000e-04]\n",
      " [4.4366e-01 4.2017e-01 5.0000e-04 4.1239e-01 5.0000e-04]\n",
      " [1.8626e-01 1.9734e-01 3.1617e-01 2.0117e-01 7.1439e-01]]\n",
      "reward -0.9697193458080579\n",
      "newState [175.97429  178.04579  207.68204  178.74713  244.4103    18.102997\n",
      "  21.45836   31.063812]\n",
      "info {'type': array([18.10299655, 21.45836018, 31.06381233])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 2\n",
      "oldState [175.97429  178.04579  207.68204  178.74713  244.4103    18.102997\n",
      "  21.45836   31.063812]\n",
      "action [[2.7804e-01 2.8442e-01 4.9769e-01 2.8633e-01 5.0000e-04]\n",
      " [4.4366e-01 4.2017e-01 5.0000e-04 4.1239e-01 5.0000e-04]\n",
      " [1.8626e-01 1.9734e-01 3.1617e-01 2.0117e-01 7.1439e-01]]\n",
      "reward -1.0989362256775466\n",
      "newState [491.9      491.9      491.9      491.9      491.9       34.76189\n",
      "  42.239338  78.882866]\n",
      "info {'type': array([34.76189034, 42.23933764, 78.88286432])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 0\n",
      "oldState [491.9       491.9       491.9       491.9       491.9         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "action [[9.93630e-01 1.01608e+00 1.76182e+00 1.02257e+00 5.00000e-04]\n",
      " [1.58660e+00 1.49345e+00 5.00000e-04 1.46271e+00 5.00000e-04]\n",
      " [6.56210e-01 7.02420e-01 1.13464e+00 7.18410e-01 7.14390e-01]]\n",
      "reward -0.057955805632930076\n",
      "newState [458.21954  457.5003   450.16006  457.25284  474.79477    6.277636\n",
      "   7.163428  17.836748]\n",
      "info {'type': array([ 6.27763623,  7.16342792, 17.83674722])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 1\n",
      "oldState [458.21954  457.5003   450.16006  457.25284  474.79477    6.277636\n",
      "   7.163428  17.836748]\n",
      "action [[9.93630e-01 1.01608e+00 1.76182e+00 1.02257e+00 5.00000e-04]\n",
      " [1.58660e+00 1.49345e+00 5.00000e-04 1.46271e+00 5.00000e-04]\n",
      " [6.56210e-01 7.02420e-01 1.13464e+00 7.18410e-01 2.54433e+00]]\n",
      "reward 0.1355008069874194\n",
      "newState [428.91174   427.89462   418.85812   427.5414    429.4055      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "info {'type': array([3.24394553, 4.69108711, 5.75323122])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 2\n",
      "oldState [428.91174   427.89462   418.85812   427.5414    429.4055      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "action [[9.93630e-01 1.01608e+00 1.76182e+00 1.02257e+00 5.00000e-04]\n",
      " [1.58660e+00 1.49345e+00 5.00000e-04 1.46271e+00 5.00000e-04]\n",
      " [6.56210e-01 7.02420e-01 1.13464e+00 7.18410e-01 2.54433e+00]]\n",
      "reward 0.18334094940758364\n",
      "newState [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "info {'type': array([ 91.38704446, 116.69336287,  95.54699897])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 0\n",
      "oldState [314.2     314.2     314.2     314.2     314.2      91.14954  63.51344\n",
      "  88.95162]\n",
      "action [[2.3649e-01 2.4295e-01 4.2835e-01 2.4494e-01 5.0000e-04]\n",
      " [3.7989e-01 3.5941e-01 5.0000e-04 3.5261e-01 5.0000e-04]\n",
      " [1.6004e-01 1.6909e-01 2.7123e-01 1.7222e-01 6.0840e-01]]\n",
      "reward -1.255250879681928\n",
      "newState [254.28012  254.18704  250.998    254.15912  260.00452   60.78364\n",
      " 111.91122    8.510616]\n",
      "info {'type': array([ 60.78364135, 111.91121293,   8.51061659])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 1\n",
      "oldState [254.28012  254.18704  250.998    254.15912  260.00452   60.78364\n",
      " 111.91122    8.510616]\n",
      "action [[2.3649e-01 2.4295e-01 4.2835e-01 2.4494e-01 5.0000e-04]\n",
      " [3.7989e-01 3.5941e-01 5.0000e-04 3.5261e-01 5.0000e-04]\n",
      " [1.6004e-01 1.6909e-01 2.7123e-01 1.7222e-01 6.0840e-01]]\n",
      "reward -1.1255908031285535\n",
      "newState [196.02942  197.75859  222.59705  198.34406  254.74031   18.102997\n",
      "  21.45836   31.063812]\n",
      "info {'type': array([18.10299655, 21.45836018, 31.06381233])}\n",
      "\n",
      "iter 0\n",
      "episode 0\n",
      "step 2\n",
      "oldState [196.02942  197.75859  222.59705  198.34406  254.74031   18.102997\n",
      "  21.45836   31.063812]\n",
      "action [[2.3649e-01 2.4295e-01 4.2835e-01 2.4494e-01 5.0000e-04]\n",
      " [3.7989e-01 3.5941e-01 5.0000e-04 3.5261e-01 5.0000e-04]\n",
      " [1.6004e-01 1.6909e-01 2.7123e-01 1.7222e-01 6.0840e-01]]\n",
      "reward -1.2548098839289752\n",
      "newState [491.9      491.9      491.9      491.9      491.9       34.76189\n",
      "  42.239338  78.882866]\n",
      "info {'type': array([34.76189034, 42.23933764, 78.88286432])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 0\n",
      "oldState [491.9       491.9       491.9       491.9       491.9         8.275917\n",
      "   6.1462727  23.933739 ]\n",
      "action [[9.91880e-01 1.01709e+00 1.76320e+00 1.02452e+00 5.00000e-04]\n",
      " [1.58867e+00 1.49321e+00 5.00000e-04 1.46197e+00 5.00000e-04]\n",
      " [6.55980e-01 7.02030e-01 1.14542e+00 7.17750e-01 6.08400e-01]]\n",
      "reward -0.06770072638103418\n",
      "newState [458.2268   457.50275  449.89062  457.25705  477.33148    6.277636\n",
      "   7.163428  17.836748]\n",
      "info {'type': array([ 6.27763623,  7.16342792, 17.83674722])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 1\n",
      "oldState [458.2268   457.50275  449.89062  457.25705  477.33148    6.277636\n",
      "   7.163428  17.836748]\n",
      "action [[9.91880e-01 1.01709e+00 1.76320e+00 1.02452e+00 5.00000e-04]\n",
      " [1.58867e+00 1.49321e+00 5.00000e-04 1.46197e+00 5.00000e-04]\n",
      " [6.55980e-01 7.02030e-01 1.14542e+00 7.17750e-01 2.53329e+00]]\n",
      "reward 0.1358669488674694\n",
      "newState [428.91928   427.89938   418.38776   427.55045   432.1391      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "info {'type': array([3.24394553, 4.69108711, 5.75323122])}\n",
      "\n",
      "iter 1\n",
      "episode 0\n",
      "step 2\n",
      "oldState [428.91928   427.89938   418.38776   427.55045   432.1391      3.2439456\n",
      "   4.6910872   5.753231 ]\n",
      "action [[9.91880e-01 1.01709e+00 1.76320e+00 1.02452e+00 5.00000e-04]\n",
      " [1.58867e+00 1.49321e+00 5.00000e-04 1.46197e+00 5.00000e-04]\n",
      " [6.55980e-01 7.02030e-01 1.14542e+00 7.17750e-01 2.53329e+00]]\n",
      "reward 0.1837069977494408\n",
      "newState [314.2     314.2     314.2     314.2     314.2      91.38705 116.69336\n",
      "  95.547  ]\n",
      "info {'type': array([ 91.38704446, 116.69336287,  95.54699897])}\n",
      "        Algorithm    Reward     Time     Space   Efficiency  Hindsight Envy  \\\n",
      "0   Guardrail-0.5 -1.694855  3.61022 -157448.0 -3459.168823       -0.642997   \n",
      "1   Guardrail-0.3 -1.453570  3.64248  -48247.5 -2945.517029       -0.844327   \n",
      "2  Guardrail-0.25 -1.691890  3.64157  -42018.5 -3047.148865       -0.838092   \n",
      "\n",
      "   Counterfactual Envy  Budget  \n",
      "0            -7.786976     0.0  \n",
      "1            -7.629428     0.0  \n",
      "2            -7.689329     0.0  \n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/resource_'+str(agent)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(resource_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(resource_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/resource_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/resource_'+str(agent)+'/')\n",
    "        algo_list_radar.append(str(agent))     \n",
    "        \n",
    "fig_path = '../figures/'\n",
    "fig_name = 'resource'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)        \n",
    "        \n",
    "additional_metric = { 'Efficiency': lambda traj : or_suite.utils.delta_EFFICIENCY(traj, CONFIG),\n",
    "                    'Hindsight Envy': lambda traj : or_suite.utils.delta_HINDSIGHT_ENVY(traj, CONFIG),\n",
    "                      'Counterfactual Envy': lambda traj : or_suite.utils.delta_COUNTERFACTUAL_ENVY(traj, CONFIG),\n",
    "                    'Budget': lambda traj : or_suite.utils.times_out_of_budget(traj, CONFIG)}\n",
    "#                       'Prop': lambda traj : or_suite.utils.delta_PROP(traj, CONFIG), \\\n",
    "#                       'Exante Envy': lambda traj : or_suite.utils.delta_EXANTE_ENVY(traj, CONFIG)}\n",
    "fig_name = 'resource'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96273425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/resource_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0a703c640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/resource_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c08c369",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"500\"\n",
       "            src=\"../figures/resource_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0b4796790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/resource_radar_plot.pdf\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
