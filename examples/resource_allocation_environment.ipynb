{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "# Resource Allocation Code Demo\n",
    "\n",
    "The Food Bank of the Southern Tier (FBST) is a member of Feeding America, focused on providing food security for people with limited financial resources, and serves six counties and nearly 4,000 square miles in the New York.  Under normal operations (non COVID times), the Mobile Food Pantry program is among the main activities of the FBST.  The goal of the service is to make nutritious and healthy food more accessible to people in underserved communities.  Even in areas where other agencies provide assistance, clients may not always have access to food due to limited public transportation options, or because those agencies are only open hours or days per work.\n",
    "\n",
    "Here we do a sample experiment testing out some of the existing and developed algorithms against a randomized heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54262089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08848504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting out configuration parameter for the environment\n",
    "CONFIG = or_suite.envs.env_configs.resource_allocation_foodbank_config(60)\n",
    "# CONFIG = or_suite.envs.env_configs.resource_allocation_default_config\n",
    "# CONFIG = or_suite.envs.env_configs.resource_allocation_simple_poisson_config\n",
    "\n",
    "\n",
    "\n",
    "# Specifying training iteration, epLen, number of episodes, and number of iterations\n",
    "\n",
    "# SIMPLE CONFIG\n",
    "# val = 500\n",
    "# CONFIG['num_rounds'] = val\n",
    "# CONFIG['init_budget'] = lambda: np.asarray([val*1.5])\n",
    "\n",
    "# DEFAULT CONFIG\n",
    "# val = 200\n",
    "# CONFIG['num_rounds'] = val\n",
    "# CONFIG['init_budget'] = lambda: 9 * val * np.ones(2)\n",
    "\n",
    "\n",
    "epLen = CONFIG['num_rounds']\n",
    "nEps = 1\n",
    "numIters = 10\n",
    "\n",
    "# Configuration parameters for running the experiment\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/resource/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, # save trajectory for calculating additional metrics\n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False # indicator for pickling final information\n",
    "                    }\n",
    "\n",
    "resource_env = gym.make('Resource-v0', config=CONFIG)\n",
    "mon_env = Monitor(resource_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive-amplifier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agents = {\n",
    " 'Equal': or_suite.agents.resource_allocation.equal_allocation.equalAllocationAgent(epLen, CONFIG),\n",
    "  'FixedThreshold': or_suite.agents.resource_allocation.fixed_threshold.fixedThresholdAgent(epLen, CONFIG),\n",
    " 'Guardrail-0.5': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.5),\n",
    "  'Guardrail-0.3': or_suite.agents.resource_allocation.hope_guardrail.hopeguardrailAgent(epLen, CONFIG, 0.3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-dublin",
   "metadata": {},
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e2e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d840813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal\n",
      "Writing to file data.csv\n",
      "FixedThreshold\n",
      "Writing to file data.csv\n",
      "Guardrail-0.5\n",
      "Writing to file data.csv\n",
      "Guardrail-0.3\n",
      "Writing to file data.csv\n",
      "        Algorithm      Reward      Time     Space     Efficiency  \\\n",
      "0           Equal -489.613698  3.256036 -164233.5 -339415.246094   \n",
      "1  FixedThreshold    1.007723  2.702367 -207993.8  -51128.100098   \n",
      "2   Guardrail-0.5    7.652270  2.196191 -200608.9   -8342.449646   \n",
      "3   Guardrail-0.3    8.662001  2.249138 -187534.8   -4382.787977   \n",
      "\n",
      "   Hindsight Envy  Counterfactual Envy  \n",
      "0     -128.730672          -117.199942  \n",
      "1       -4.029401            -1.768414  \n",
      "2       -5.797603            -1.568582  \n",
      "3       -9.789596            -4.359215  \n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/resource_'+str(agent)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(resource_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(resource_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/resource_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/resource_'+str(agent)+'/')\n",
    "        algo_list_radar.append(str(agent))     \n",
    "        \n",
    "fig_path = '../figures/'\n",
    "fig_name = 'resource'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)        \n",
    "        \n",
    "additional_metric = { 'Efficiency': lambda traj : or_suite.utils.delta_EFFICIENCY(traj, CONFIG),\n",
    "                    'Hindsight Envy': lambda traj : or_suite.utils.delta_HINDSIGHT_ENVY(traj, CONFIG),\n",
    "                      'Counterfactual Envy': lambda traj : or_suite.utils.delta_COUNTERFACTUAL_ENVY(traj, CONFIG),\n",
    "#                     'Budget': lambda traj : or_suite.utils.times_out_of_budget(traj, CONFIG)\n",
    "#                       'Prop': lambda traj : or_suite.utils.delta_PROP(traj, CONFIG), \\\n",
    "#                       'Exante Envy': lambda traj : or_suite.utils.delta_EXANTE_ENVY(traj, CONFIG)\n",
    "                    }\n",
    "fig_name = 'resource'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c08c369",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mIFrame\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../figures/resource_radar_plot.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IFrame' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/resource_radar_plot.pdf\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
