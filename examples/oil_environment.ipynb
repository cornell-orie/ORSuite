{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "# Oil Environment Code Demo\n",
    "\n",
    "This problem, adaptved from [here](https://www.pnas.org/content/109/3/764) is a continuous variant of the “Grid World” environment. It comprises of an agent surveying a d-dimensional map in search of hidden “oil deposits”. The world is endowed with an unknown survey function which encodes the probability of observing oil at that specific location. For agents to move to a new location they pay a cost proportional to the distance moved, and surveying the land produces noisy estimates of the true value of that location. In addition, due to varying terrain the true location the agent moves to is perturbed as a function of the state and action.\n",
    "\n",
    "\n",
    "There is a $d$-dimensional reinforcement learning environment in the space $X = [0, 1]^d$.  The action space $A = [0,1]^d$ corresponding to the ability to attempt to move to any desired location within the state space.  On top of that, there is a corresponding reward function $f_h(x,a)$ for the reward for moving the agent to that location.  Moving also causes an additional cost $\\alpha d(x,a)$ scaling with respect to the distance moved.\n",
    "\n",
    "In this notebook we run a sample experiment for the setting when $d = 1$ and the reward function is taken to be a quadratic.  We compare several heuristics to existing reinforcement learning algorithms.\n",
    "\n",
    "Here is an example illustrating the problem on a 1 dimensional line:\n",
    "\n",
    "![Oil_Line_Diagram](diagrams/oil_line_diagram.png)\n",
    "    \n",
    "* Assuming a reasonable cost to move, the agent will likely want to move towards the right. If the cost to move is heavily penalized, the agent could chose to stay in place or possibly move to the left.\n",
    "* Exactly how far the agent moves will be determined by the cost to move\n",
    "* Finally, the agent may not end up exactly at its target location, as affected by the “terrain”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa4e6",
   "metadata": {},
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54262089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-burst",
   "metadata": {},
   "source": [
    "### Experimental Parameters\n",
    "\n",
    "\n",
    "The oil discovery problem has several experiment parameters\n",
    "* The parameter `epLen`, an int, number of time steps to run the experiment for \n",
    "* `nEps` is an int representing the number of episodes. Here it is set to 300. \n",
    "* `numIters`, an int, is the number of iterations. Here it is set to 30. \n",
    "* `seed` allows random numbers to be generated. \n",
    "* `dirPath`, a string, is the location where the data files are stored.\n",
    "* `deBug`, a bool, prints information to the command line when set true. \n",
    "* `save_trajectory`, a bool, saves the trajectory information of the simulation when set to true. \n",
    "* `render` renders the algorithm when set to true.\n",
    "* `pickle` is a bool that saves the information to a pickle file when set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG =  or_suite.envs.env_configs.oil_environment_default_config\n",
    "CONFIG =  or_suite.envs.env_configs.oil_environment_binary_config # to work with bin search agent\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 300\n",
    "numIters = 3\n",
    "\n",
    "epsilon = (nEps * epLen)**(-1 / 4)\n",
    "action_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "state_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "\n",
    "scaling_list = [0.1, 0.3, 1, 5]\n",
    "\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/oil/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14f148",
   "metadata": {},
   "source": [
    "### Specifying Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a6ea6",
   "metadata": {},
   "source": [
    "We specify 6 different agents to compare the effectiveness of each.\n",
    "\n",
    "* `SB PPO` is Proximal Policy Optimization. When policy is updated, there is a parameter that “clips” each policy update so that action update does not go too far\n",
    "* `Random` implements the randomized RL algorithm, which selects an action uniformly at random from the action space. In particular, the algorithm stores an internal copy of the environment’s action space and samples uniformly at random from it.\n",
    "* `AdaQL` is an Adaptive Discretization Model-Free Agent, implemented for enviroments with continuous states and actions using the metric induced by the l_inf norm.\n",
    "* `AdaMB` is an Adaptive Discretizaiton Model-Based Agent, implemented for enviroments with continuous states and actions using the metric induced by the l_inf norm.\n",
    "* `Unif QL` is an eNet Model-Based Agent, implemented for enviroments with continuous states and actions using the metric induces by the l_inf norm.\n",
    "* `Unif MB` is a eNet Model-Free Agent, implemented for enviroments with continuous states and actions using the metric induces by the l_inf norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb85100a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oil_env = gym.make('Oil-v0', config=CONFIG)\n",
    "mon_env = Monitor(oil_env)\n",
    "dim = CONFIG['dim']\n",
    "cost_param = CONFIG['cost_param']\n",
    "prob = CONFIG['oil_prob']\n",
    "\n",
    "agents = {# 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'Binary': or_suite.agents.oil_discovery.grid_search.grid_searchAgent(epLen, dim), \n",
    "# 'AdaQL': or_suite.agents.rl.ada_ql.AdaptiveDiscretizationQL(epLen, scaling_list[0], True, dim*2),\n",
    "# 'AdaMB': or_suite.agents.rl.ada_mb.AdaptiveDiscretizationMB(epLen, scaling_list[0], 0, 2, True, True, dim, dim),\n",
    "# 'Unif QL': or_suite.agents.rl.enet_ql.eNetQL(action_net, state_net, epLen, scaling_list[0], (dim,dim)),\n",
    "# 'Unif MB': or_suite.agents.rl.enet_mb.eNetMB(action_net, state_net, epLen, scaling_list[0], (dim,dim), 0, False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3cf90",
   "metadata": {},
   "source": [
    "### Running Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290adfbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Writing to file data.csv\n",
      "Binary\n",
      "Writing to file data.csv\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(oil_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(oil_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__))\n",
    "        algo_list_radar.append(str(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1104535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Algorithm    Reward      Time   Space\n",
      "0    Random  3.393350  6.512343 -3646.0\n",
      "1    Binary  4.440593  6.732720 -3460.0\n"
     ]
    }
   ],
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'oil_metric'+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'oil_metric'+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788d47d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/oil_metric_1_0_<lambda>_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f94d3b367c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/oil_metric_1_0_<lambda>_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4bcde62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"450\"\n",
       "            src=\"../figures/oil_metric_1_0_<lambda>_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f94c753e3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/oil_metric_1_0_<lambda>_radar_plot.pdf\", width=600, height=450)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
