{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experienced-income",
   "metadata": {},
   "source": [
    "# Oil Environment Code Demo\n",
    "\n",
    "This problem, adaptved from [here](https://www.pnas.org/content/109/3/764) is a continuous variant of the “Grid World” environment. It comprises of an agent surveying a d-dimensional map in search of hidden “oil deposits”. The world is endowed with an unknown survey function which encodes the probability of observing oil at that specific location. For agents to move to a new location they pay a cost proportional to the distance moved, and surveying the land produces noisy estimates of the true value of that location. In addition, due to varying terrain the true location the agent moves to is perturbed as a function of the state and action.\n",
    "\n",
    "\n",
    "There is a $d$-dimensional reinforcement learning environment in the space $X = [0, 1]^d$.  The action space $A = [0,1]^d$ corresponding to the ability to attempt to move to any desired location within the state space.  On top of that, there is a corresponding reward function $f_h(x,a)$ for the reward for moving the agent to that location.  Moving also causes an additional cost $\\alpha d(x,a)$ scaling with respect to the distance moved.\n",
    "\n",
    "Here is an example of the oil discovery problem:\n",
    "![](diagams/oil_line_diagram.png)\n",
    "\n",
    "In this notebook we run a sample experiment for the setting when $d = 1$ and the reward function is taken to be a quadratic.  We compare several heuristics to existing reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9aa4e6",
   "metadata": {},
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54262089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb26c67",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-burst",
   "metadata": {},
   "source": [
    "Here we use the oil environment as outlined in `or_suite/envs/oil_discovery/oil_environment.py`.  The package has default specifications for all of the environments in the file `or_suite/envs/env_configs.py`, and so we use one the defaults.\n",
    "\n",
    "In addition, we need to specify the number of episodes for learning, and the number of iterations (in order to plot average results with confidence intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG =  or_suite.envs.env_configs.oil_environment_default_config\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 50\n",
    "numIters = 2\n",
    "\n",
    "epsilon = (nEps * epLen)**(-1 / 4)\n",
    "action_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "state_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "\n",
    "scaling_list = [0.1, 0.3, 1, 5]\n",
    "\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/oil/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe14f148",
   "metadata": {},
   "source": [
    "### Specifying Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a6ea6",
   "metadata": {},
   "source": [
    "We specify 6 different agents to compare the effectiveness of each.\n",
    "\n",
    "* `SB PPO` is Proximal Policy Optimization. When policy is updated, there is a parameter that “clips” each policy update so that action update does not go too far\n",
    "* `Random` implements the randomized RL algorithm, which selects an action uniformly at random from the action space. In particular, the algorithm stores an internal copy of the environment’s action space and samples uniformly at random from it.\n",
    "* `AdaQL` is an Adaptive Discretization Model-Free Agent, implemented for enviroments with continuous states and actions using the metric induced by the l_inf norm.\n",
    "* `AdaMB` is an Adaptive Discretizaiton Model-Based Agent, implemented for enviroments with continuous states and actions using the metric induced by the l_inf norm.\n",
    "* `Unif QL` is an eNet Model-Based Agent, implemented for enviroments with continuous states and actions using the metric induces by the l_inf norm.\n",
    "* `Unif MB` is a eNet Model-Free Agent, implemented for enviroments with continuous states and actions using the metric induces by the l_inf norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb85100a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jren/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:131: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5\n",
      "We recommend using a `batch_size` that is a multiple of `n_steps * n_envs`.\n",
      "Info: (n_steps=5 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oil_env = gym.make('Oil-v0', config=CONFIG)\n",
    "mon_env = Monitor(oil_env)\n",
    "dim = CONFIG['dim']\n",
    "cost_param = CONFIG['cost_param']\n",
    "prob = CONFIG['oil_prob']\n",
    "\n",
    "agents = { 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'AdaQL': or_suite.agents.rl.ada_ql.AdaptiveDiscretizationQL(epLen, scaling_list[0], True, dim*2),\n",
    "'AdaMB': or_suite.agents.rl.ada_mb.AdaptiveDiscretizationMB(epLen, scaling_list[0], 0, 2, True, True, dim, dim),\n",
    "'Unif QL': or_suite.agents.rl.enet_ql.eNetQL(action_net, state_net, epLen, scaling_list[0], (dim,dim)),\n",
    "'Unif MB': or_suite.agents.rl.enet_mb.eNetMB(action_net, state_net, epLen, scaling_list[0], (dim,dim), 0, False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3cf90",
   "metadata": {},
   "source": [
    "### Running Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290adfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB PPO\n",
      "New Experiment Run\n",
      "Iteration: 0\n",
      "Iteration: 1\n",
      "[3.6317595512196768, 3.9479653408136524, 4.129764065402788, 4.5681063138742335, 2.6719079712822915, 3.103638323514327, 3.7492616925947533, 2.4715177646857693, 4.4047920937662335, 4.367879441171443, 4.089239260077029, 3.4082598973485667, 3.512701904127055, 3.572068297683658, 3.660571575421462, 4.063370970372511, 2.7069188992454394, 3.224249761517296, 4.238426907630147, 3.249807310752513, 3.639336018042334, 4.133878668253824, 3.5737416455354376, 3.588760657922095, 3.8054863855497314, 4.820314473011692, 4.129442713557884, 3.68176897211678, 4.220494337502176, 5.0, 4.484483922954664, 4.29460156918473, 5.0, 4.244449923236082, 3.7357588823428847, 4.215857209053706, 4.35561353507409, 4.332794316386925, 4.4157133722276685, 5.0, 3.49324760548126, 5.0, 4.1266238918946225, 3.989809919360368, 5.0, 3.2632563024005323, 4.548014494421658, 4.251749102967782, 5.0, 5.0, 5.0, 3.4513324705421518, 4.731503782330988, 4.394491927104884, 5.0, 5.0, 4.469095128222921, 4.934454631009965, 4.5037327525436055, 3.6897120795615637, 3.827912369836818, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "100\n",
      "Writing to file ../data/oil_metric_SB PPO_1_0_<lambda>/data.csv\n",
      "Random\n",
      "Writing to file data.csv\n",
      "AdaQL\n",
      "Chosen parameters: 0.1\n",
      "Writing to file data.csv\n",
      "0.1\n",
      "AdaMB\n",
      "Chosen parameters: 0.3\n",
      "Writing to file data.csv\n",
      "0.3\n",
      "Unif QL\n",
      "Chosen parameters: 0.1\n",
      "Writing to file data.csv\n",
      "0.1\n",
      "Unif MB\n",
      "Chosen parameters: 0.1\n",
      "Writing to file data.csv\n",
      "0.1\n",
      "  Algorithm    Reward      Time   Space\n",
      "0    Random  3.558955  6.511445 -4030.0\n",
      "1     AdaQL  4.528145  5.890690 -4468.0\n",
      "2     AdaMB  4.725755  5.970240 -3604.0\n",
      "3   Unif QL  4.290420  6.025425 -3536.0\n",
      "4   Unif MB  5.000000  5.348450 -3400.0\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'/'\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(oil_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(oil_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/oil_metric_'+str(agent)+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__))\n",
    "        algo_list_radar.append(str(agent))\n",
    "\n",
    "fig_path = '../figures/'\n",
    "fig_name = 'oil_metric'+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'oil_metric'+'_'+str(dim)+'_'+str(cost_param)+'_'+str(prob.__name__)+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a13ae",
   "metadata": {},
   "source": [
    "Here we see the uniform discretization model based algorithm performs the best with a minimal time complexity for evaluating the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
