{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb87f4c",
   "metadata": {},
   "source": [
    "# Revenue Management Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80672f",
   "metadata": {},
   "source": [
    "The revenue management problem entails the management of different available resources consumed by different classes of customers in order to maximize revenue. The environment contains an agent that must determine what class of customers to accept at different locations for revenue, through careful allocation to avoid exhaustion of resources. The agent also has to consider the probabilities of customers showing up to the system. If a customer arrives to the system and is accepted by the agent, then the customer utilizes some amount of the different resources and provides some amount of revenue.\n",
    "\n",
    "The state space of the environment is the amount of available resources for the agent and is represented by $S = [0,B_1]*[0,B_2]*...*[0,B_k]$, where $B_i$ is the max availability of resource $i$ and $k$ is the total number of resources. The action space of the environment is a binary vector of length n that determines, which classes of customers are accepted and rejected. This is represented by $A = [0,...,1]$ with length $n$, where $n$ represents the number of customer classes. Additionally, the reward for the agent is just the revenue from selling resources to customer class that arrives, and the reward is zero if the customer was denied or resources are not available. \n",
    "\n",
    "The state transitions based on arrival $P_t$ that either equals $j_t \\in [n]$ or $\\emptyset$: \n",
    "* If $P_t = \\emptyset$, then no arrivals occured, $reward = 0$, and $S_t = S_{t+1}$. \n",
    "* If $P_t = j_t$ and $a_{jt} = 0$, then arrivals were rejected, $reward = 0$, and $S_t = S_{t+1}$. \n",
    "* If $P_t = j_t$, $a_{jt} = 1$, and $S_t-A_{jt}^T > 0$ (resources purchased), then arrivals were accepted and enough resources were available such that $S_{t+1} = S_t - A_{jt}^T$ with $reward = f_{jt}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c274f0",
   "metadata": {},
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8403b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46358d50",
   "metadata": {},
   "source": [
    "### Experimental Parameters\n",
    "\n",
    "\n",
    "The revenue management problem has several experiment parameters\n",
    "* The parameter `epLen`, an int, number of time steps to run the experiment for \n",
    "* `nEps` is an int representing the number of episodes. The default is set to 2. \n",
    "* `numIters`, an int, is the number of iterations. Here it is set to 50. \n",
    "* `seed` allows random numbers to be generated. \n",
    "* `dirPath`, a string, is the location where the data files are stored.\n",
    "* `deBug`, a bool, prints information to the command line when set true. \n",
    "* `save_trajectory`, a bool, saves the trajectory information of the simulation when set to true. \n",
    "* `render` renders the algorithm when set to true.\n",
    "* `pickle` is a bool that saves the information to a pickle file when set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23629db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG =  or_suite.envs.env_configs.airline_default_config\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 2\n",
    "numIters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33db72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "\n",
    "revenue_env = gym.make('Airline-v0', config=CONFIG)\n",
    "mon_env = Monitor(revenue_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd3e9e",
   "metadata": {},
   "source": [
    "### Specifying Agent\n",
    "\n",
    "We specify 4 agents to compare effectiveness of each:\n",
    "\n",
    "* `SB PPO` is Proximal Policy Optimization. When policy is updated, there is a parameter that “clips” each policy update so that action update does not go too far\n",
    "* `Random` is a randomized RL algorithm, which randomly selects whether to accept/reject customer classes.\n",
    "* `BayesSelector` is an optimization algorithm, which determines what optimal actions to take based on current inventory levels and the expected number of future arrivals (`RoundFlag` = True)\n",
    "    * (`RoundFlag` = True) - Allocate based on the proportion of types accepted across all rounds being larger than 1/2\n",
    "* `BayesSelectorBadRounding` is similar to the `BayesSelector` agent, but instead it's rounding is more inaccurate (`RoundFlag` = False)\n",
    "    * (`RoundFlag` = False) - Allocate with a random policy which allocates a type to a bernoulli sampled from the proportion of those types accepted across all rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c431f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epLen=100\n",
    "agents = { #'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'BayesSelector': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=True),\n",
    "'BayesSelectorBadRounding': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a69d84",
   "metadata": {},
   "source": [
    "### Experiment Set-Up\n",
    "\n",
    "In each experiment we set up parameters for our environment. This includes setting up the config dictionary that sets up the revenue management environment.\n",
    "We then set the desired number of episodes and number of iterations. Then, we set up the settings for running the experiment, by creating the `DEFAULT_SETTINGS` dictionary. We then create an instance of the environment and a monitor for it. \n",
    "    Then, the experiment is run by calling `run_single_algo` (or `run_single_sb_algo` for the SB PPO agent). The results of the experiment are written to a csv file which can be used to obtain the line plot and radar graphs for each agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2020234",
   "metadata": {},
   "source": [
    "## Basic Simulation\n",
    "\n",
    "In this example, we use the default configuration and 50 iterations with 2 episodes. This is a synthetic example with 2 classes where each class has a 1 in 3 chance of arriving (and there is a 1/3 chance that no class arrives). There are also 3 types of resources available for the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44feb1d7",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147e79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'BayesSelector': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=True),\n",
    "'BayesSelectorBadRounding': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e4db8",
   "metadata": {},
   "source": [
    "### Running Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee56bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Writing to file data.csv\n",
      "BayesSelector\n",
      "Writing to file data.csv\n",
      "BayesSelectorBadRounding\n",
      "Writing to file data.csv\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/airline_'+str(agent)\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(revenue_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/airline_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/airline_'+str(agent))\n",
    "        algo_list_radar.append(str(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75d4360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Algorithm  Reward      Time     Space\n",
      "0                    Random    1.72  6.770615  -3727.14\n",
      "1             BayesSelector    2.72  2.849841 -28247.98\n",
      "2  BayesSelectorBadRounding    2.88  2.845717 -26245.44\n"
     ]
    }
   ],
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'revenue'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'revenue'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9317108d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/revenue_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa9c71b2070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"../figures/revenue_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd86f4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"450\"\n",
       "            src=\"../figures/revenue_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7faa20114a30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/revenue_radar_plot.pdf\", width=600, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1211fd",
   "metadata": {},
   "source": [
    "### Results\n",
    "Based on the table above, the Bayes Selector agent outperforms the Random Agent. The 'Bad Rounding' version of the Bayes Selector agent performs slightly worse, but is still very close to the normal version. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f74e6f",
   "metadata": {},
   "source": [
    "## Simulation with Dual Degeneracies\n",
    "\n",
    "In this example, we use the default configuration. This is a synthetic example with 2 classes where each class has a 1 in 3 chance of arriving (and there is a 1/3 chance that no class arrives). There are also 3 types of resources available for the customers.  The difference with the previous, though, is that the cost parameters are sampled such that the solution experiences dual degeneracy (see [here](https://arxiv.org/abs/1906.06361) for a discussion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4999",
   "metadata": {},
   "source": [
    "### Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cd88139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = .45 # either do .44 or .45\n",
    "CONFIG['P'] = np.asarray([[1-p, p],[1-p,p],[1-p,p],[1-p,p],[1-p,p]])\n",
    "\n",
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "\n",
    "revenue_env = gym.make('Airline-v0', config=CONFIG)\n",
    "mon_env = Monitor(revenue_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070b794",
   "metadata": {},
   "source": [
    "### Specifying Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bd6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'BayesSelector': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen),\n",
    "'BayesSelectorBadRounding': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463b7e6",
   "metadata": {},
   "source": [
    "### Running Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b0a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Writing to file data.csv\n",
      "BayesSelector\n",
      "Writing to file data.csv\n",
      "BayesSelectorBadRounding\n",
      "Writing to file data.csv\n",
      "                  Algorithm  Reward      Time     Space\n",
      "0                    Random    2.32  6.583894  -3610.36\n",
      "1             BayesSelector    3.14  2.966289 -27388.78\n",
      "2  BayesSelectorBadRounding    3.14  3.022140 -26070.02\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/airline_'+str(agent)\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(revenue_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/airline_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/airline_'+str(agent))\n",
    "        algo_list_radar.append(str(agent))\n",
    "        \n",
    "        \n",
    "fig_path = '../figures/'\n",
    "fig_name = 'revenue'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "# \n",
    "additional_metric = {}\n",
    "fig_name = 'revenue'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66e99594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/revenue_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa9c5010f70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/revenue_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ca9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"450\"\n",
       "            src=\"../figures/revenue_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa9c509cc70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/revenue_radar_plot.pdf\", width=600, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c65d9f",
   "metadata": {},
   "source": [
    "### Results\n",
    "Once again, the Bayes Selector agent outperforms the Random Agent. The 'Bad Rounding' version of the Bayes Selector agent performs slightly worse, but is still very close to the normal version. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61fccb",
   "metadata": {},
   "source": [
    "## Simulation with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63542ac0",
   "metadata": {},
   "source": [
    "The following parameters come from [this paper](https://courses.cit.cornell.edu/orie6590/projects/spring_2021/sam_tan.pdf) written by ORIE 6590 students. This custom policy should  be a nontrivial example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae3f31",
   "metadata": {},
   "source": [
    "### Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a472e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "epLen = 4\n",
    "A = np.asarray([[1, 1, 0,0,0,0], [ 0,0, 1, 1, 1, 1], [ 0,0, 0,0, 1, 1] ])\n",
    "tau = 23\n",
    "P = np.ones((tau, A.shape[1]))/3\n",
    "c = [5, 5, 5]\n",
    "f = range(10, 16)\n",
    "CONFIG = {'A': A, 'f': f, 'P': P, 'starting_state': c , 'tau': tau}\n",
    "nEps = 2\n",
    "numIters = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a575ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 6\n",
    "l = 3\n",
    "A = np.identity(m)\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        if i != j:\n",
    "            demand_col = np.zeros((m, 1))\n",
    "            demand_col[2 * i + 1] = 1.0\n",
    "            demand_col[2 * j] = 1.0\n",
    "            A=  np.append(A, demand_col, axis = 1)\n",
    "A = np.append(A, A, axis = 1)\n",
    "tau = 20\n",
    "P = np.array([0.01327884, 0.02244177, 0.07923761, 0.0297121,  0.02654582, 0.08408091, 0.09591975, 0.00671065, 0.08147508, 0.00977341, 0.02966204, 0.121162, 0.00442628, 0.00748059, 0.02641254, 0.00990403, 0.00884861, 0.02802697, 0.03197325, 0.00223688, 0.02715836, 0.0032578,  0.00988735, 0.04038733])\n",
    "P = np.array([P]*tau)\n",
    "c = [2]*6\n",
    "f = np.array([33, 28, 36, 34, 17, 20, 39, 24, 31, 19, 30, 48, 165, 140, 180, 170, 85, 100,195, 120, 155, 95, 150, 240])\n",
    "CONFIG = {'epLen':epLen, 'A': A, 'f': f, 'P': P, 'starting_state': c , 'tau': tau}\n",
    "epLen = CONFIG['epLen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c5277e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "\n",
    "revenue_env = gym.make('Airline-v0', config=CONFIG)\n",
    "mon_env = Monitor(revenue_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85764d",
   "metadata": {},
   "source": [
    "### Specifying Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f1810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'BayesSelector': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen),\n",
    "'BayesSelectorBadRounding': or_suite.agents.airline_revenue_management.bayes_selector.bayes_selectorAgent(epLen, round_flag=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff896c",
   "metadata": {},
   "source": [
    "### Running Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c660973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Writing to file data.csv\n",
      "BayesSelector\n",
      "Writing to file data.csv\n",
      "BayesSelectorBadRounding\n",
      "Writing to file data.csv\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/airline_'+str(agent)\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(revenue_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/airline_'+str(agent))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/airline_'+str(agent))\n",
    "        algo_list_radar.append(str(agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395b77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Algorithm  Reward      Time     Space\n",
      "0                    Random  120.02  5.851176  -3786.72\n",
      "1             BayesSelector  134.54  2.714183 -27144.78\n",
      "2  BayesSelectorBadRounding  126.76  2.647503 -26858.46\n"
     ]
    }
   ],
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'revenue'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "# \n",
    "additional_metric = {}\n",
    "fig_name = 'revenue'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93583b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"280\"\n",
       "            src=\"../figures/revenue_line_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa9c4e82850>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/revenue_line_plot.pdf\", width=600, height=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fe7d1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"450\"\n",
       "            src=\"../figures/revenue_radar_plot.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa9c4ebafd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"../figures/revenue_radar_plot.pdf\", width=600, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecababb",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once again, the Bayes Selector agents outperform the Random agent and accumulate a higher reward. However, for this set of parameters, the \"Bad Rounding\" agent accumulates a slightly higher reward than the normal Bayess Selector agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518c74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
