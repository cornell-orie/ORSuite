{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d221b48e",
   "metadata": {},
   "source": [
    "# The Oil Discovery Problem\n",
    "\n",
    "## Description\n",
    "\n",
    "This problem, adaptved from [here](https://www.pnas.org/content/109/3/764) is a continuous variant of the “Grid World” environment. It comprises of an agent surveying a d-dimensional map in search of hidden “oil deposits”. The world is endowed with an unknown survey function which encodes the probability of observing oil at that specific location. For agents to move to a new location they pay a cost proportional to the distance moved, and surveying the land produces noisy estimates of the true value of that location. In addition, due to varying terrain the true location the agent moves to is perturbed as a function of the state and action.\n",
    "\n",
    "\n",
    "`oil_problem.py` is a $d$-dimensional reinforcement learning environment in the space $X = [0, 1]^d$.  The action space $A = [0,1]^d$ corresponding to the ability to attempt to move to any desired location within the state space.  On top of that, there is a corresponding reward function $f_h(x,a)$ for the reward for moving the agent to that location.  Moving also causes an additional cost $\\alpha d(x,a)$ scaling with respect to the distance moved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727855e",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "\n",
    "### State Space\n",
    "\n",
    "\n",
    "The state space for the line environment is $S = X^d$ where $X = [0, 1]$ and there are $d$ dimensions.\n",
    "\n",
    "\n",
    "### Action space\n",
    "\n",
    "\n",
    "The agent chooses a location to move to, and so the action space is also $A = X^d$ where $X = [0,1]$ and there are $d$ dimensions.\n",
    "\n",
    "\n",
    "\n",
    "### Reward\n",
    "\n",
    "The reward is $\\text{oil prob}(s, a, h) - \\alpha \\sum_i |s_i - a_i|$ where $s$ is the previous state of the system, $a$ is the action chosen by the user, $\\text{oil prob}$ is a user specified reward function, and $\\alpha$ dictates the cost tradeoff for movement.  Clearly when $\\alpha = 0$ then the optimal policy is to just take the action that maximizes the resulting oil probability function.\n",
    "\n",
    "The $\\alpha$ parameter though more generally allows the user to control how much to penalize the agent for moving.\n",
    "\n",
    "\n",
    "### Transitions\n",
    "\n",
    "Given an initial state at the start of the iteration $s$, an action chosen by the user $a$, the next state will be \n",
    "$\\begin{align*}\n",
    "    s_{new} = a + \\text{Normal}(0, \\sigma(s,a,h))\n",
    "\\end{align*}$\n",
    "where $\\sigma(s,a,h)$ is a user-specified function corresponding to the variance in movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ee546",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "### Metric\n",
    "\n",
    "`reset`\n",
    "\n",
    "Returns the environment to its original state.\n",
    "\n",
    "`step(action)`\n",
    "\n",
    "Takes an action from the agent and returns the state of the system.\n",
    "\n",
    "\n",
    "Returns:\n",
    "\n",
    "* `state`: A list containing the new location of the agent\n",
    "\n",
    "* `reward`: The reward associated with the most recent action and event\n",
    "\n",
    "* `pContinue`:\n",
    "\n",
    "* `info`: empty\n",
    "\n",
    "`render`\n",
    "\n",
    "Currently unimplemented\n",
    "\n",
    "\n",
    "`close`\n",
    "\n",
    "Currently unimplemented\n",
    "\n",
    "\n",
    "Init parameters for the line ambulance environment, passed in using a dictionary named CONFIG\n",
    "\n",
    "* `epLen`: the length of each episode\n",
    "\n",
    "* `dim`: the dimension of the problem\n",
    "\n",
    "* `alpha`: a float $\\in [0,1]$ that controls the proportional difference between the cost to move\n",
    "\n",
    "* `oil_prob`: a function corresponding to the reward for moving to a new location\n",
    "\n",
    "* `noise_variance`: a function corresponding to the variance for movement\n",
    "\n",
    "* `starting_state`: an element in $[0,1]^{dim}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76e602",
   "metadata": {},
   "source": [
    "## Heuristic Agents\n",
    "\n",
    "### Binary Search Agent\n",
    "\n",
    "The binary search agent uses the binary search algorithm to close in on the location with the greatest probability of finding oil. The agent keeps record of the location of the upper and lower bounds of its search area, which is initialized as the boundaries of the metrix space $X$. In each dimension, the agent calculates the midpoint and makes small perturbations towards either bound, and records the direction of highest reward. The agent then adjusts its upper and lower bounds towards the direction of highest reward, and the algorithm is repeated until the algorithm converges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
